# FET-GAN

This is the webpage of the paper:

Li W, He Y, Qi Y, Li Z, Tang Y. FET-GAN: Font Effect Transfer via K-shot Adaptive Instance Normalization[C //Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34.

<p align="center">
  <img width="400" src="./docs/imgs/func.png">
</p>

It is provided for educational/research purpose only. Please consider citing our paper if you find it useful for your work.

## Abstract

Text effect transfer aims at learning the mapping between text visual effects while maintaining the text content. 
While remarkably successful, existing methods have limited robustness in font transfer and weak generalization ability to unseen effects. 
To address these problems, we propose FET-GAN, a novel end-to-end framework to implement visual effects transfer with font variation among multiple text effects domains. 
Our model achieves remarkable results both on arbitrary effect transfer between texts and effect translation from text to graphic objects. 
By a few-shot fine-tuning strategy, FET-GAN can generalize the transfer of the pre-trained model to the new effect. 
Through extensive experimental validation and comparison, our model advances the state-of-the-art in the text effect transfer task. 
Besides, we have collected a font dataset including 100 fonts of more than 800 Chinese and English characters. 
Based on this dataset, we demonstrated the generalization ability of our model by the application that complements the font library automatically by few-shot samples. 
This application is significant in reducing the labor cost for the font designer. 

## Presentation Video (Youtube)

<!--[![Youtube](http://img.youtube.com/vi/txYmA5ePDOM/0.jpg)](http://www.youtube.com/watch?v=txYmA5ePDOM "AAAI 2020 oral presentation FET-GAN")-->

<p align="center">
	<a target="_blank" href="http://www.youtube.com/watch?v=txYmA5ePDOM">
		<img width="640" src="http://img.youtube.com/vi/txYmA5ePDOM/0.jpg">
	<a/>  
</p>

<iframe width="640" height="360" src="https://www.youtube.com/embed/txYmA5ePDOM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Experimental Results
![](./docs/imgs/contrast.png)

## Download

### Paper
[coming soon]

### Webpage

<ul>
	<li>
		<a href="https://liweileev.github.io/FET-GAN/">
			<img src="./docs/imgs/jekyll.png" height="60" alt="Github page">
		</a>
	</li>
</ul>

### Code
* <a target="_blank" href="https://github.com/liweileev/FET-GAN"><img src="./docs/imgs/github.png" width=45  alt="Github code"></a>

### Pre-trained models

<ul>
	<li>
		<p>
			<a href="https://drive.google.com/drive/folders/13kqa8miU97IMsIyM-KpBmr1kd8nHWUJy">
				<img src="./docs/imgs/GoogleDrive.svg?sanitize=true" width="45" alt="Google Drive Datasets">
			</a>
		</p>
	</li>
	<li>
		<p>
			<a href="https://pan.baidu.com/s/1403BzONK60QSf0v2aoRNFg">
				<img src="./docs/imgs/BaiduDrive.png" width="45" alt="Baidu Drive  Datasets">
			</a>
		</p>
	</li>
</ul>

### Datasets

![](./docs/imgs/Fonts-100.png)

* <a target="_blank" href="https://drive.google.com/open?id=1OcOSTg29IY9UDCEB2gL4d3ALpUyvzD-2"><img src="./docs/imgs/GoogleDrive.svg" width=45 alt="Google Drive Datasets"></a>

* <a target="_blank" href="https://pan.baidu.com/s/1xhKpuSqHWxLlll9Rwf_7cA"><img src="./docs/imgs/BaiduDrive.png" width=45 alt="Baidu Drive  Datasets"></a>

## How to Use

### Installation


<!--## Citation-->



